---
title: "Lab 3 Demo"
author: "Mateo Robbins"
date: "2023-01-24"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(rsample)
library(skimr)
library(glmnet)
```

## Data Wrangling and Exploration
```{r data}
#load and inspect the data
dat <- AmesHousing::make_ames()
```

##Train a model
```{r intial_split}
# Data splitting with {rsample} 
set.seed(123) #set a seed for reproducibility
split <- initial_split(dat) 
# shows amount of obs. in training/testing/total
split 

# pull train and test data from the split
ames_train <- training(split)
ames_test  <- testing(split)
```

```{r model_data}
#Create training feature matrices using model.matrix() (auto encoding of categorical variables)

# specify outcome variable Sale_Price (what we trying to predict); use "." to indicate we want all of the other variables from the training data
X <- model.matrix(Sale_Price ~ ., data = ames_train)[,-1] #remove intercept variable (mateo doesnt know why it's in the data set)

# run if you want to look at the data distributions for all of the variables
#skim(dat)

# transform y with log() transformation since it's skewed
Y <- log(ames_train$Sale_Price)

```

```{r glmnet}
#fit a ridge model, passing X,Y,alpha to glmnet()
ridge <- glmnet(
  x = X,
  y = Y,
  alpha = 0 # tell the function if you want lasso (1) or ridge (0); any value in between 0 and 1 will be elastic net
)

#plot() the glmnet model object
plot(ridge, xvar = "lambda")  
```

```{r}
# lambdas applied to penalty parameter.  Examine the first few
ridge$lambda %>%
  head()

# small lambda results in large coefficients
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"),100] # 100th value of lambda 
# location (lat) seems to be a more important variable than the quality of the property

# what about for small coefficients?
coef(ridge)[c("Latitude", "Overall_QualVery_Excellent"),1] # increase value of lambda (value of lambda going from large to small in the matrix) 
# now we have very small coefficients
  
```
How much improvement to our loss function as lambda changes?

##Tuning
```{r cv.glmnet}
# Apply CV ridge regression to Ames data.  Same arguments as before to glmnet()
ridge <- cv.glmnet(
  x = X,
  y = Y,
  alpha = 0 # specify ridge
)

# resample data, cross validation (CV)

# Apply CV lasso regression to Ames data
lasso <- cv.glmnet( 
  x = X,
  y = Y,
  alpha = 1 # specify lasso
  )

# cross validation to see how well model will perform on unseen data
  
# plot results
par(mfrow = c(1, 2))
# see how well the model is performing within the folds
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```

- looking at mean squared error
- as lambda increase MSE is changing
- We want MSE to be low
   - want small values of lambda for this model 
   - by containing coefficients, we are hurting the model
- first vertical line on plot
  - which value of lambda gives the lowest MSE ()
- Second vertical line:
  - **1 standard error rule**: trade off between MSE and parsimony in the model (don't want to rely only on MSE bc its and estimate)
  - Want to pick the point for lambda that gives the most parsimonious model, while staying in 1 SE
  - **parsimony**: use fewer variables
  - lasso is already removing variables when they are pushed to zero; we let the model choose the variables
- this is a 10 fold run, so the grey lines around the red points is the variability since the red is the average
  
10-fold CV MSE for a ridge and lasso model. What's the "rule of 1 standard error"?
- ^^ see notes above

In both models we see a slight improvement in the MSE as our penalty log(Î») gets larger, suggesting that a regular OLS model likely overfits the training data. But as we constrain it further (i.e., continue to increase the penalty), our MSE starts to increase. 

Let's examine the important parameter values apparent in the plots.
```{r}
# Ridge model ..............
# minimum MSE
min(ridge$cvm) 
# lambda for this min MSE (first dotted vertical line from the plot)
ridge$lambda.min

# 1-SE rule 
ridge$cvm[ridge$lambda == ridge$lambda.1se]
# lambda for this MSE (second dotted vertical line from the plot)
ridge$lambda.1se


# Lasso model .............
# minimum MSE
min(lasso$cvm)
# lambda for this min MSE
lasso$lambda.min

# 1-SE rule
lasso$cvm[lasso$lambda == lasso$lambda.1se]
# lambda for this MSE
lasso$lambda.1se

# No. of coef | 1-SE MSE
lasso$nzero[lasso$lambda == lasso$lambda.1se] # the number of predictors in the model

# can compare to the minimum MSE method
lasso$nzero[lasso$lambda == lasso$lambda.min] 

# We reduced the number of predictors by ~1/2
```

```{r}
# Ridge model
ridge_min 

# Lasso model
lasso_min


par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
```

