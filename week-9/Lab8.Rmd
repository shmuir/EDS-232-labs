---
title: "Lab 8"
author: "Sam Muir"
date: "2024-03-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
library(tidymodels)
library(dplyr)
library(kernlab)
library(tidyverse)
```

## Forest Cover Classification with SVM

In this week's lab we are exploring the use of Support Vector Machines for multi-class classification. Specifically, you will be using cartographic variables to predict forest cover type (7 types).

Natural resource managers responsible for developing ecosystem management strategies require basic descriptive information including inventory data like forest cover type for forested lands to support their decision-making processes. However, managers generally do not have this type of data for in-holdings or neighboring lands that are outside their immediate jurisdiction. One method of obtaining this information is through the use of predictive models.

You task is build both an SVM and a random forest model and compare their performance on accuracy and computation time.

1.  The data is available here: <https://ucsb.box.com/s/ai5ost029enlguqyyn04bnlfaqmp8kn4>

Explore the data.

```{r}
forest <- read_csv(here::here("week-9", "covtype_sample.csv")) %>%
  janitor::clean_names() %>%
  mutate(cover_type = as.factor(cover_type)) %>%
  mutate(across(6:54, as.factor)) %>%
  select(-soil_type_15) # only have value 
#skimr::skim(forest)
```

```{r}
ggplot(forest, aes(x = cover_type)) +
  geom_histogram(stat = "count") +
  theme_bw()
```

-   What kinds of features are we working with?

**We have some topographical features that are continuous variables (slope, aspect) and some that are binary (the soil types).**

-   Does anything stand out that will affect you modeling choices?

**Some of the continuous variables are not normally distributed. There is also an imbalance in frequency between the cover types.**

Hint: Pay special attention to the distribution of the outcome variable across the classes.

2.  Create the recipe and carry out any necessary preprocessing. Can you use the same recipe for both models?

3.  Create the folds for cross-validation.

4.  Tune the models. Choose appropriate parameters and grids. If the computational costs of tuning given your strategy are prohibitive, how might you work around this?

5.  Conduct final predictions for both models and compare their prediction performances and computation costs from part 4.

## SVM Poly

### Create the recipe and carry out any necessary preprocessing.
```{r}
# Split the data into training and testing
split <- initial_split(forest, 0.7, strata = cover_type)
forest_training <- training(split)
forest_testing <- testing(split)

# create recipe
svm_rec <- recipe(cover_type~., data = forest_training) %>%
  step_center(all_numeric_predictors()) %>%
  step_scale(all_numeric_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors())

svm_linear_spec <- svm_poly(degree = 1,
                            cost = tune()) %>% # cost parameter: width of margin around hyper plane
  set_mode("classification") %>%
  set_engine("kernlab")

svm_linear_wf <- workflow() %>% 
  add_model(svm_linear_spec %>% 
              set_args(cost_tune())) %>% 
  add_formula(cover_type~.)
#svm_linear_wf
```

### Create the folds for cross-validation.
```{r}
set.seed(258)
forest_fold <- vfold_cv(forest_training, strata = cover_type, v = 5)
```

### Tune the models. Choose appropriate parameters and grids.
```{r}
doParallel::registerDoParallel(cores = 4) # set up parallel processing for computation efficiency

param_grid <- grid_regular(cost(), levels = 5) # grid; 5 for run-time
```

```{r, eval=FALSE}
# tune grid
system.time(
tune_res <- tune_grid(
  svm_linear_wf,
  resamples = forest_fold,
  grid = param_grid
))
#      user    system   elapsed 
# 9934.777  483.132  902.373 
```

```{r, echo=FALSE}
#write_rds(tune_res, "week-9/svm_tune.rda")
tune_res <- read_rds(here::here("week-9", "svm_tune.rda"))
```

### Conduct final predictions
```{r}
autoplot(tune_res) +
  theme_bw()

## finalize 
best_cost <- select_best(tune_res, metric = "accuracy")
best_cost

svm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)
```

```{r, eval=FALSE}
svm_linear_fit <- svm_linear_final %>% fit(forest_training)
svm_linear_augment <- augment(svm_linear_fit, new_data = forest_testing) 
```

```{r, echo=FALSE}
#write_rds(svm_linear_fit, "week-9/svm_linear_fit.rda")
svm_linear_fit <- read_rds(here::here("week-9", "svm_linear_fit.rda"))

# augment 
#write_rds(svm_linear_augment, "week-9/svm_linear_augment.rda")
svm_linear_augment <- read_rds(here::here("week-9/svm_linear_augment.rda"))
```

```{r}
svm_linear_augment %>%
  conf_mat(truth = cover_type, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

svm_linear_augment %>%
  roc_curve(truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7) %>%
  autoplot()

svm_linear_augment %>%
  roc_auc(truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7)
```


## SVM RBF
```{r}
#svm rbf --------------------
svm_rbf_spec <- svm_rbf() %>%
  set_mode("classification") %>%
  set_engine("kernlab")
```

```{r, eval=FALSE}
system.time(
svm_rbf_fit <- svm_rbf_spec %>%
  fit(cover_type~., data = forest_training))
#    user  system elapsed 
# 299.652  14.191 315.562 

svm_rbf_augment <- augment(svm_rbf_fit, new_data = forest_testing)
```

```{r, echo=FALSE}
#write_rds(svm_rbf_fit, "week-9/svm_rbf_fit.rda")
svm_rbf_fit <- read_rds(here::here("week-9/svm_rbf_fit.rda"))
#write_rds(svm_rbf_augment, "week-9/svm_rbf_augment.rda")
svm_rbf_augment <- read_rds(here::here("week-9/svm_rbf_augment.rda"))
```

```{r}
svm_rbf_augment %>%
  conf_mat(truth = cover_type, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

#We can examine our model's performance using ROC and AUC
svm_rbf_augment %>%
  roc_curve(., truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7) %>%
  autoplot()

svm_rbf_augment %>%
  roc_auc(truth = cover_type, .pred_1, .pred_2, .pred_3, .pred_4, .pred_5, .pred_6, .pred_7)
```

## Random Forest
```{r}
# splitting into a much smaller training set for computational time
second_split <- initial_split(forest_training, 0.05)
second_training <- training(second_split)
second_testing <- testing(second_split)
```

### Create the recipe and carry out any necessary preprocessing.
```{r}
# create a recipe
rf_recipe <- recipe(cover_type ~ ., data = second_training) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% #normalize numeric to make sure scale is okay
  prep()

rf_model <- rand_forest(mtry = tune(),
                  trees = tune()) %>%
  set_engine("ranger") %>%
  set_mode("classification")

# workflow ----
rf_workflow <- workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_recipe)

forest_fold <- vfold_cv(second_training, strata = cover_type, v = 2)
```

### Tune the models. Choose appropriate parameters and grids.
```{r, eval=FALSE}
# parameter tuning ----
system.time(
  rf_cv_tune <- rf_workflow %>%
  tune_grid(resamples = forest_fold, grid = 3)) #use cross validation to tune mtry and trees parameters
#    user  system elapsed 
# 528.482   7.170 542.422 
```

```{r, echo=FALSE}
#write_rds(rf_cv_tune, "week-9/rf_cv_tune.rda")
rf_cv_tune <- read_rds(here::here("week-9/rf_cv_tune.rda"))
```

### Conduct final predictions
```{r}
#get metrics from tuning cv to pick best model ----
#collect_metrics(rf_cv_tune) 

#plot cv results for parameter tuning ----
# autoplot(rf_cv_tune) + 
#   theme_bw()

# finalize workflow ----
rf_best <- show_best(rf_cv_tune, n = 1, metric = "roc_auc") #get metrics for best random forest model
#rf_best

rf_final <- finalize_workflow(rf_workflow,
                              select_best(rf_cv_tune, metric = "roc_auc"))

# model fitting ----
train_fit_rf <- fit(rf_final, second_training) #fit the model to the training set
#train_fit_rf

# prediction probabilities ----
test_predict_rf <- predict(train_fit_rf, second_testing, type = "prob") %>% #get testing prediction
  bind_cols(second_testing) %>%  #bind to testing column
  filter(cover_type != 4) %>%
  mutate(cover_type = as.numeric(cover_type),
         cover_type = as.factor(cover_type))

# roc auc curve
roc_curve(test_predict_rf, cover_type, .pred_1, .pred_2, .pred_3, .pred_5, .pred_6, .pred_7) %>%
  autoplot() 

test_predict_rf %>%
  roc_auc(cover_type, .pred_1, .pred_2, .pred_3, .pred_5, .pred_6, .pred_7)
```

-   Which type of model do you think is better for this task?

**I think that for this task the RBF SVM is better for this task, especially in terms of computational efficiency. It took the Poly SVM a while to run with all of the training data, but the random forest also took a while to run with only 5% of the training data. In comparison, the RBF SVM took a fraction of the tine with all of the training data. The roc_auc for the models were comparable, though the random forest roc_auc was a bit higher than the RBF SVM, which was slightly higher than the Poly SVM. In the grand scheme of machine learning I know this isn't the largest data set, but for our purpose, I would probably use the RBF SVM to balance computational efficiency and performance.**

-   Why do you speculate this is the case?

**I think that the random forest had better performance since it is better for multi-class classification where SVM is better for two-class. For the RBF SVM, I did not specify a grid or folds, so this reduces computation time, but also reduces model performance.**

