---
title: "Lab 8 Demo"
author: "Mateo Robbins"
date: "2024-03-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
library(dplyr)
library(kernlab)
```


```{r}
#Create simulate training data for our SVM exercise
set.seed(1)

sim_data <- tibble(
  x1 = rnorm(40),
  x2 = rnorm(40),
  y = factor(rep(c(-1,1), 20))) %>%
  mutate(x1 = ifelse(y==1, x1 + 1.5, x1),
         x2 = ifelse(y==1, x2 + 1.5, x2))
  
```

```{r}
#plot to see the structure of the data we created
ggplot(sim_data, aes(x1, x2, color = y)) +
  geom_point()
```

```{r svm_rec}
#specify a recipe where we center to mean of 0 and scale to sd of 1
svm_rec <- recipe(y~., data = sim_data) %>%
  step_center(all_predictors()) %>%
  step_scale(all_predictors())
```

```{r svm_spec}
#Create linear SVM model specification
# polynomial kernel
svm_linear_spec <- svm_poly(degree = 1,
                            cost = 10) %>% # cost parameter: width of margin around hyper plane
  set_mode("classification") %>%
  set_engine("kernlab")
```

In SVM, the cost parameter influences the width of the margin around the separating hyperplane. A smaller C allows a wider margin but more misclassifications are allowed. Recall that we can improve  generalization by accepting more errors on the training set. A larger C aims for a narrower margin that tries to correctly classify as many training samples as possible, even if it means a more complex model.

```{r}
#Bundle into workflow
svm_workflow <- workflow() %>%
  add_recipe(svm_rec) %>%
  add_model(svm_linear_spec)
```

```{r}
#Fit workflow
svm_linear_fit <- fit(svm_workflow, data = sim_data) 
```

```{r}
#Plot the fit from kernlab engine
svm_linear_fit %>% 
  extract_fit_engine() %>% 
  plot()
```
- decision boundary is where it is the whitest
- shapes are the two values of the factor y
- showing points closest to the decision boundary; further away from the decision line is more certain

```{r tune}
#As usual we want to tune our hyperparameter values
svm_linear_spec <- svm_poly(degree = 1,
                            cost = tune()) %>% # cost parameter: width of margin around hyper plane
  set_mode("classification") %>%
  set_engine("kernlab")

svm_linear_wf <- workflow() %>% 
  add_model(svm_linear_spec %>% set_args(cost_tune())) %>% 
  add_formula(y~.)
svm_linear_wf
```

```{r}
set.seed(258)
sim_data_fold <- vfold_cv(sim_data, strata = y)

param_grid <- grid_regular(cost(), levels = 10)

# tune grid
tune_res <- tune_grid(
  svm_linear_wf,
  resamples = sim_data_fold,
  grid = param_grid
)

autoplot(tune_res)
```


#Finalize model and fit
```{r finalize}
best_cost <- select_best(tune_res, metric = "accuracy")
best_cost

svm_linear_final <- finalize_workflow(svm_linear_wf, best_cost)

svm_linear_fit <- svm_linear_final %>% fit(sim_data)
svm_linear_fit
```


```{r sim_test}
#Create a small test data set
set.seed(2)

sim_data_test <- tibble(
  x1 = rnorm(40),
  x2 = rnorm(40),
  y = factor(rep(c(-1,1), 20))) %>%
  mutate(x1 = ifelse(y==1, x1 + 1.5, x1),
         x2 = ifelse(y==1, x2 + 1.5, x2))
```

We can use augment() from {broom} to use our trained model to predict on new data (test data) and add additional info for examining model performance. 

```{r augment}
augment(svm_linear_fit, new_data = sim_data_test) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

That went well, but makes SVMs really interesting is that we can use non-linear kernels. Let us start by generating some data, but this time generate with a non-linear class boundary.

```{r}
set.seed(789)
sim_data2 <- tibble(
  x1 = rnorm(200) + rep(c(2,-2,0), c(100,50,50)),
  x2 = rnorm(200) + rep(c(2,-2,0), c(100,50,50)),
  y = factor(rep(c(1,2), c(150,50)))
)
```

```{r svm_rbf}
svm_rbf_spec <- svm_rbf() %>% 
  set_mode("classification") %>%
  set_engine("kernlab")
```


```{r}
#Fit the new specification
svm_rbf_fit <- svm_rbf_spec %>%
  fit(y~., data = sim_data2)

svm_rbf_fit
```

```{r}
#Plot the fit
svm_rbf_fit %>% extract_fit_engine() %>%
  plot()
```

```{r}
#Create the test data
set.seed(465)

sim_data2_test <- tibble(
  x1 = rnorm(200) + rep(c(2,-2,0), c(100,50,50)),
  x2 = rnorm(200) + rep(c(2,-2,0), c(100,50,50)),
  y = factor(rep(c(1,2), c(150,50)))
)
```

```{r}
#Examine model performance via confusion matrix
augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  conf_mat(truth = y, estimate = .pred_class)
```

ROC Curves

```{r}
#We can examine our model's performance using ROC and AUC
augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  roc_curve(truth = y, .pred_1) %>%
  autoplot()

augment(svm_rbf_fit, new_data = sim_data2_test) %>%
  roc_auc(truth = y, .pred_1)
```
