---
title: "Lab6"
author: "Sam Muir"
date: "2023-03-03"
output: html_document
---

## Case Study: Eel Distribution Modeling

This week's lab follows a project modeling the eel species Anguilla australis described by Elith et al. (2008). There are two data sets for this lab.  You'll use one for training and evaluating your model, and you'll use your model to make predictions predictions on the other.  Then you'll compare your model's performance to the model used by Elith et al.

## Data

Grab the training data sets (eel.model.data.csv, eel.eval.data.csv) from github here:
https://github.com/MaRo406/eds-232-machine-learning/blob/main/data 

```{r, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```


```{r}
library(tidyverse)
library(skimr)
library(tidymodels)
library(caret)
library(patchwork)
library(rsample)
```

```{r}
eel_model_data <- read_csv(here::here("week-7/eel.model.data.csv")) %>%
  mutate(Angaus = as.factor(Angaus)) %>%
  select(-Site)
eel_eval_data <- read_csv(here::here("week-7/eel.eval.data.csv")) %>%
  rename(Angaus = Angaus_obs) %>%
  mutate(Angaus = as.factor(Angaus))
```

### Split and Resample

Split the model data (eel.model.data.csv) into a training and test set, stratified by outcome score (Angaus). Use 10-fold CV to resample the training set.

```{r}
set.seed(222)
# split data
eel_split <- initial_split(eel_model_data, strata = Angaus)
eel_training <- training(eel_split)
eel_testing <- testing(eel_split)

#set up k-fold cv
eel_cv <- eel_training %>%
  vfold_cv(v=10, strata = Angaus)
#eel_cv
```


### Preprocess

Create a recipe to prepare your data for the XGBoost model

```{r}
eel_recipe <- recipe(Angaus ~ ., data = eel_training) %>%
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>% # dummy variables from all factors
  step_normalize(all_numeric_predictors()) # normalize all numeric predictors
```


## Tuning XGBoost

### Tune Learning Rate

Following the XGBoost tuning strategy outlined in lecture, first we conduct tuning on just the learning rate parameter:

1.  Create a model specification using {xgboost} for the estimation

-   Only specify one parameter to tune()

```{r}
eel_spec_lr_tune <- boost_tree(trees = 3000, learn_rate = tune()) %>% # set trees; standard value 
  set_engine("xgboost") %>%
  set_mode("classification")

eel_lr_tune_wf <- workflow() %>% #create workflow
  add_model(eel_spec_lr_tune) %>%
  add_recipe(eel_recipe)
```


2.  Set up a grid to tune your model by using a range of learning rate parameter values: expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

-   Use appropriate metrics argument(s) - Computational efficiency becomes a factor as models get more complex and data get larger. Record the time it takes to run. Do this for each tuning phase you run.You could use {tictoc} or Sys.time().

```{r}
# set up grid
lr_grid <- expand.grid(learn_rate = seq(0.0001, 0.3, length.out = 30))

#set up code to run using parallel processing
doParallel::registerDoParallel() 

# tune lr
system.time( # timer
  eel_cv_tune <- eel_lr_tune_wf %>%
    tune_grid(resamples = eel_cv, 
              grid = lr_grid, # specify grid
              metrics = metric_set(accuracy, roc_auc, pr_auc))
  )
```


3.  Show the performance of the best models and the estimates for the learning rate parameter values associated with each.

```{r}
show_best(eel_cv_tune, metric = "roc_auc")
```


### Tune Tree Parameters

1.  Create a new specification where you set the learning rate (which you already optimized) and tune the tree parameters.

```{r}
# XGBoost model spec
eel_tree_param_spec <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 3000,
    min_n = tune(),
    tree_depth = tune(),
    loss_reduction = tune(),
    learn_rate = select_best(eel_cv_tune, metric = "roc_auc")$learn_rate) %>% # select optimized learn rate
    set_engine("xgboost")
```


2.  Set up a tuning grid. This time use grid_latin_hypercube() to get a representative sampling of the parameter space

```{r}
# grid specification
xgboost_tree_params <- 
  dials::parameters(
    min_n(),
    tree_depth(),
    loss_reduction())

# grid_latin_hypercube
xgboost_tree_params_grid <- 
  dials::grid_latin_hypercube( 
    xgboost_tree_params,  
    size = 50 #number of parameter value combos 
  )

xgboost_tree_params_wf <- 
  workflows::workflow() %>%
  add_model(eel_tree_param_spec) %>% 
  add_recipe(eel_recipe)

# tune hyperparameters
system.time(
  xgboost_tree_params_tuned <- tune::tune_grid(
  object = xgboost_tree_params_wf,
  resamples = eel_cv,
  grid = xgboost_tree_params_grid,
  metrics = yardstick::metric_set(accuracy, roc_auc, pr_auc))
  )

```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
show_best(xgboost_tree_params_tuned, metric = "roc_auc")
```


### Tune Stochastic Parameters

1.  Create a new specification where you set the learning rate and tree parameters (which you already optimized) and tune the stochastic parameters.

```{r}
# XGBoost model specification
eel_stochastic_spec <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 3000,
    min_n = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$min_n, # min # of points in a node that is required for node to be split further
    tree_depth = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$tree_depth, # max depth of tree; number of splits
    learn_rate = select_best(eel_cv_tune, metric = "roc_auc")$learn_rate, # rate at which the boosting algorithm adapts from iteration-to-iteration
    loss_reduction = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$loss_reduction, # the reduction in the loss function required to split further
    mtry = tune(), # number predictors sampled at each split
    sample_size = tune(), # amount of data exposed to the fitting routine
    stop_iter = tune()) %>% # number of iterations without improvement before stopping 
    set_engine("xgboost")
```


2.  Set up a tuning grid. Use grid_latin_hypercube() again.

```{r}
# grid specification
xgboost_stochastic_params <- dials::parameters(
  finalize(mtry(), select(eel_training, -Angaus)),
  sample_size = sample_prop(),
  stop_iter())

xgboost_stochastic_grid <- dials::grid_latin_hypercube(
  xgboost_stochastic_params, 
  size = 50)

#create workflow
xgboost_stochastic_wf <- 
  workflows::workflow() %>%
  add_model(eel_stochastic_spec) %>% 
  add_recipe(eel_recipe)

# hyperparameter tuning
system.time(
  xgboost_stochastic_tuned <- tune::tune_grid(
  object = xgboost_stochastic_wf,
  resamples = eel_cv,
  grid = xgboost_stochastic_grid,
  metrics = yardstick::metric_set(accuracy, roc_auc, pr_auc))
)
```


3.  Show the performance of the best models and the estimates for the tree parameter values associated with each.

```{r}
show_best(xgboost_stochastic_tuned, metric = "roc_auc")
```


## Finalize workflow and make final prediction

1.  How well did your model perform? What types of errors did it make?

```{r}
# creating final model
full_model_spec <- 
  parsnip::boost_tree(
    mode = "classification",
    trees = 3000,
    min_n = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$min_n,
    tree_depth = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$tree_depth,
    learn_rate = select_best(eel_cv_tune, metric = "roc_auc")$learn_rate,
    mtry = select_best(xgboost_stochastic_tuned, metric = "roc_auc")$mtry,
    loss_reduction = select_best(xgboost_tree_params_tuned, metric = "roc_auc")$loss_reduction,
    sample_size = select_best(xgboost_stochastic_tuned, metric = "roc_auc")$sample_size,
    stop_iter = select_best(xgboost_stochastic_tuned, metric = "roc_auc")$stop_iter) %>%
    set_engine("xgboost")
```

```{r}
# final workflow
final_wf <- workflow() %>%
  add_recipe(eel_recipe) %>%
  add_model(full_model_spec)

set.seed(457)

# last fit
final_fit <- last_fit(final_wf, eel_split)

#final_fit %>% collect_metrics()

test_eel_preds <- final_fit$.predictions[[1]]

# set up confusion matrix 
conf_matrix <- test_eel_preds %>% yardstick::conf_mat(truth=Angaus, estimate=.pred_class) 

# plot conf mat.
autoplot(conf_matrix, type = "heatmap") + 
  labs(title = "Confusion Matrix for Test Data")
```

**Overall the model did pretty well. It had a ~83% accuracy. The majority of the errors were false negatives (predicted not present when it actually was), though there were some false positives as well.**

## Fit your model the evaluation data and compare performance

1.  Now used your final model to predict on the other dataset (eval.data.csv)

```{r}
# fit the final model
set.seed(123)
eels_eval_fit <- fit(final_wf, data = eel_eval_data) 

eel_eval_preds <- predict(eels_eval_fit, new_data = eel_eval_data)

eels_eval_set <- bind_cols(eel_eval_data$Angaus, eel_eval_preds) %>%
  rename(Angaus = ...1)
```


2.  How does your model perform on this data?

```{r}
# confusion matrix 
conf_matrix <- eels_eval_set %>% yardstick::conf_mat(truth=Angaus, estimate=.pred_class) 

autoplot(conf_matrix, type = "heatmap") + 
  labs(title = "Confusion Matrix for Evaluation Data")
```


**The model also performed okay on this data, with a ~88% accuracy. Again, the majority of the errors were false negatives with 75% of the errors being this kind.**


3.  How do your results compare to those of Elith et al.?

```{r}
# getting roc auc 

# get true values; needs to be numeric
true <- as.numeric(eels_eval_set$Angaus) - 1 # minus 1 bc values are 1 & 2 when set to numeric

# get pred values
predictions <- as.numeric(eels_eval_set$.pred_class) - 1

# get roc
roc_obj <- pROC::roc(true, predictions)

#calculate roc auc 
pROC::auc(roc_obj)
```

**The roc auc I calculated was worse than the one that the paper authors found (0.858). By adjusting the grids, I could also possibly improve the auc.**

-   Use {vip} to compare variable importance

```{r}
eels_eval_fit %>%
  vip::vip(geom = "point") +
  theme_bw()
```

**From my model, summer air temperature (SegSumT), distance to coast (DSDist), and average slope in the upstream catchment (USSlope) were the most important predictors. Summer air temperature was also the most important predictor for the model from the paper.**

-   What do your variable importance results tell you about the distribution of this eel species?

**These variable importance results tell us that the summer air temperature has the biggest effect on Angaus occurrence, that distance to coast has the second highest influence, and that average slope in the upstream catchment has the third highest influence. In general, sites where the Angaus eel was detected were likely to be warmer, closer to the coast, and have a milder slope in the upstream catchment .**
